{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization\n",
    "\n",
    "- compute_cost 함수에 regularization을 위한 부분 추가\n",
    "- gradient 계산에 변경된 부분 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization을 위한 가중치 lambda 튜닝\n",
    "\n",
    "- lambda가 0일 때 점수가 0.9점으로 가장 높았고 lambda값을 키울수록 점수가 낮아지는 것으로 보아 이 모델은 overfitting이 되어있지 않았다고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-129-78fc683d5810>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  enc.fit(y[:,np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-fd377ab0d802>:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  Y = enc.transform(y[:,np.newaxis]).toarray()\n"
     ]
    }
   ],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lamb):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1)) + (lamb * 0.5 * np.linalg.norm(W) ** 2)\n",
    "    return cost\n",
    "\n",
    "# + (0.5) * (W.T @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    \n",
    "    lamb = 0.5\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        # w 뒷 부분이 gradient 계산 부분\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch) + lamb * W)\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, lamb)\n",
    "        if i % 1000 == 0:\n",
    "            print(cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "# T = y_train\n",
    "\n",
    "# K = np.size(T, 1)\n",
    "# M = np.size(X, 1)\n",
    "# W = np.zeros((M,K))\n",
    "\n",
    "# iterations = 50000\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# initial_cost = compute_cost(X, T, W)\n",
    "# print(initial_cost)\n",
    "\n",
    "#print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "#(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.3024850979937166 \n",
      "\n",
      "2.2827187120330423\n",
      "3.3800105618636906\n",
      "4.9959607494039835\n",
      "6.043634767127019\n",
      "6.7458471273084974\n",
      "7.169421370643374\n",
      "7.674860354557423\n",
      "7.823776966884948\n",
      "8.138082058237092\n",
      "8.317466965461742\n",
      "8.53864958448371\n",
      "8.63489740090746\n",
      "8.68638432324448\n",
      "8.745857123470007\n",
      "8.785015462244697\n",
      "8.979236925894398\n",
      "9.062519305990566\n",
      "8.902882513958327\n",
      "9.069262256087905\n",
      "8.996625655502104\n",
      "8.97630454559014\n",
      "9.068843409178278\n",
      "9.060354789284876\n",
      "9.098343044773472\n",
      "9.112138734963114\n",
      "8.958269988244723\n",
      "9.209524035856916\n",
      "9.283111214158616\n",
      "9.306469027503832\n",
      "9.268199373661458\n",
      "9.387220049240932\n",
      "9.330178738991481\n",
      "9.316643172946698\n",
      "9.175694783295343\n",
      "9.166058112529667\n",
      "9.163391785282062\n",
      "9.255940957532317\n",
      "9.407530356483022\n",
      "9.328625478527062\n",
      "9.376926026522522\n",
      "9.33462851573931\n",
      "9.423758549257066\n",
      "9.147655954492981\n",
      "9.224748057358166\n",
      "9.31815070969632\n",
      "9.303611934385728\n",
      "9.274414363199604\n",
      "9.170391505578818\n",
      "9.471512421588274\n",
      "9.280225933281107\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W, 0.5)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
